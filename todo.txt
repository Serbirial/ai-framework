since im now running the framework on my Pi cluster (512m/512m/2gb)
ill need to do:
device 1 (social tone and intents) https://huggingface.co/j-hartmann/emotion-english-distilroberta-base classify_social_tone determine_moods_from_social_classification
device 2 (actions, etc, anything not social) NO MODEL YET ?? 

i need to actuall do all the actions ActionResult

USE THIS MODEL FOR SUMMARIZING WEB CONTENT: TinyT5 (e.g. t5-small, quantized)	



Offload code gen to something like: https://huggingface.co/Salesforce/codegen-350M-multi


### TODO: Implement In-Memory Caching for Persona Prompt Generation

- Cache the result of `get_persona_prompt(userid)` to avoid regenerating the persona prompt on every call.
- Store cached prompts in a dictionary keyed by `userid`.
- Invalidate (clear) the cached prompt for a user whenever their personality data or core memory changes, including:
  - After `add_to_remember()` modifies the user's memory.
  - After updates to personality traits, goals, likes, or dislikes (need to detect or trigger cache clearing).
- Provide a method to manually clear the cache for a specific user or all users.



### TODO: Implement Chunk Summarization and Step Compression in RecursiveThinker

- Enable `RecursiveThinker` to handle inputs larger than the modelâ€™s context window by summarizing in chunks.
- When analyzing large documents (e.g., research papers or codebases), recursively:
  - Split the input into chunks small enough to fit the context window. (if too big)
  - Summarize each chunk separately using existing prompt structure.
  - Load summaries and use them as the context.
- Track total token usage during recursive steps:
  - If the combined token size of all prior steps approaches the model limit (e.g., 85% of context window), summarize all prior steps into a condensed block.
- Inject the summary seamlessly and continue generating new thought steps based on it.

Optional:
- Add recursive summarization
- Tag summarized blocks clearly so the final answer prompt can interpret them appropriately.